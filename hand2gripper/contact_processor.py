import os
import cv2
import numpy as np
import mediapy
import tqdm
from pathlib import Path
from typing import Union, List, Tuple

from hand2gripper_haco import HACOContactEstimator
from processor_config import ContactProcessorConfig, DataManager
from utils.common import read_color_image, read_depth_image, _to_numpy

class ContactProcessor:
    def __init__(self, contact_processor_config: ContactProcessorConfig, data_manager: DataManager):
        # config
        self.contact_processor_config = contact_processor_config
        self.output_dir = self.contact_processor_config.processor_output_dir
        self.data_manager = data_manager
        # contact estimator
        self.contact_estimator = HACOContactEstimator(backbone=self.contact_processor_config.backbone, checkpoint_path=self.contact_processor_config.checkpoint_path, experiment_dir=self.contact_processor_config.log_dir)
        # vis
        self.vis_contact_rendered = self.contact_processor_config.vis_contact_rendered
        self.vis_crop_img = self.contact_processor_config.vis_crop_img
        # save
        self.vis_contact_rendered_images_dir = self.contact_processor_config.vis_contact_rendered_images_dir
        self.vis_crop_img_images_dir = self.contact_processor_config.vis_crop_img_images_dir
        self.contact_processor_results_dir = self.contact_processor_config.contact_processor_results_dir

    def _process_single_sample(self, sample_id: int, color_image: Union[str, np.ndarray], depth_image: Union[str, np.ndarray]) -> np.ndarray:
        # read image
        color = read_color_image(color_image)
        depth = read_depth_image(depth_image)

        # read_bbox (generated by HandProcessor)
        bboxes = self.data_manager._read_bbox(sample_id)         # [array([592,  57, 803, 383], dtype=int32)]
        bboxes = self._xyxy_to_xywh(bboxes)         # [array([592,  57, 211, 326], dtype=int32)]
        is_right = self.data_manager._read_is_right(sample_id)   # [array(False)]
        img_size = self.data_manager._read_img_size(sample_id)   # array([1080, 1080], dtype=int32)

        # predict contact
        ## contact_results: Dictionary containing contact results {'contact_mask': np.ndarray, 'contact_rendered': np.ndarray, 'crop_img': np.ndarray, 'raw_outputs': dict}
        ## contact_mask: (778,), contact_rendered: (H, W, 3), crop_img: (h, w, 3), raw_outputs: dict
        ## raw_outputs: Dictionary containing raw outputs {'contact_out': Tensor(1, 778), 'contact_336_out': Tensor(1, 336), 'contact_84_out': Tensor(1, 84), 'contact_joint_out': Tensor(1, 21)}
        for idx in range(len(bboxes)):
            # distinguish right and left hand
            if is_right[idx]:
                contact_results = self.contact_estimator.predict_contact(color, bboxes[idx])
            else:
                color_flip, bbox_flip = self._flip_color_and_bbox(color, bboxes[idx], img_size)
                fake_right_contact_results = self.contact_estimator.predict_contact(color_flip, bbox_flip)
                contact_results = self._flip_contact_results_right_to_left(fake_right_contact_results)
            # vis
            if self.vis_contact_rendered:
                self._vis_contact_rendered_images(sample_id, idx, contact_results['contact_rendered'], self.vis_contact_rendered_images_dir)
            if self.vis_crop_img:
                self._vis_crop_img_images(sample_id, idx, contact_results['crop_img'], self.vis_crop_img_images_dir)
            # save
            self._save_contact_results(sample_id, idx, contact_results, self.contact_processor_results_dir)

    def _vis_contact_rendered_images(self, sample_id: int, hand_id: int, contact_rendered: np.ndarray, save_dir: str):
        os.makedirs(save_dir, exist_ok=True)
        if contact_rendered.dtype != np.uint8:
            if contact_rendered.max() <= 1.0:
                contact_rendered = (contact_rendered * 255).astype(np.uint8)
            else:
                contact_rendered = contact_rendered.astype(np.uint8)
        cv2.imwrite(os.path.join(save_dir, f"{sample_id}_{hand_id}.png"), contact_rendered)

    def _vis_crop_img_images(self, sample_id: int, hand_id: int, crop_img: np.ndarray, save_dir: str):
        os.makedirs(save_dir, exist_ok=True)
        if crop_img.dtype != np.uint8:
            if crop_img.max() <= 1.0:
                crop_img = (crop_img * 255).astype(np.uint8)
            else:
                crop_img = crop_img.astype(np.uint8)
        cv2.imwrite(os.path.join(save_dir, f"{sample_id}_{hand_id}.png"), crop_img)

    def _xyxy_to_xywh(self, bboxes: List[np.ndarray]) -> List[np.ndarray]:
        xywh_bboxes = []
        for bbox in bboxes:
            xywh_bbox = np.array([bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]])
            xywh_bboxes.append(xywh_bbox)
        return xywh_bboxes
    
    def _flip_color_and_bbox(self, color: np.ndarray, bbox_xywh: np.ndarray, img_size: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        color_flip = cv2.flip(color, 1)
        W, H = img_size[0], img_size[1]
        x, y, w, h = bbox_xywh

        bbox_xywh_flip = np.array([
            W - x - w,
            y,
            w,
            h
        ], dtype=bbox_xywh.dtype)
        
        return color_flip, bbox_xywh_flip
    
    def _flip_contact_results_right_to_left(self, contact_results: dict) -> dict:
        contact_results_flip = {}
        # flip vis images
        contact_results_flip['crop_img'] = cv2.flip(contact_results['crop_img'], 1)
        contact_results_flip['contact_rendered'] = cv2.flip(contact_results['contact_rendered'], 1)
        # by default, the joint order of the left and right hands is the same
        contact_results_flip['raw_outputs'] = contact_results['raw_outputs']

        return contact_results_flip
    
    def _save_contact_results(self, sample_id: int, hand_id: int, contact_results: dict, save_dir: str):
        os.makedirs(save_dir, exist_ok=True)
        data = {
            'contact_out': _to_numpy(contact_results['raw_outputs']['contact_out'][0]),
            'contact_336_out': _to_numpy(contact_results['raw_outputs']['contact_336_out'][0]),
            'contact_84_out': _to_numpy(contact_results['raw_outputs']['contact_84_out'][0]),
            'contact_joint_out': _to_numpy(contact_results['raw_outputs']['contact_joint_out'][0]),
        }
        out_path = os.path.join(save_dir, f"{sample_id}_{hand_id}.npz")
        np.savez_compressed(out_path, **data)


if __name__ == '__main__':
    raw_samples_root_dir = "/home/yutian/projs/Hand2Gripper/hand2gripper/raw"
    for samples_id in os.listdir(raw_samples_root_dir):
        if os.path.isdir(os.path.join(raw_samples_root_dir, samples_id)):
            contact_processor_config = ContactProcessorConfig(samples_id)
            data_manager = DataManager(samples_id)
            contact_processor = ContactProcessor(contact_processor_config, data_manager)
            video_path = os.path.join(raw_samples_root_dir, samples_id, "video.mp4")
            depth_npy_path = os.path.join(raw_samples_root_dir, samples_id, "depth.npy")
            video = mediapy.read_video(video_path)
            depth_npy = np.load(depth_npy_path)  # meters
            assert len(video) == len(depth_npy), "Number of frames in video and depth image must be the same"
            for idx in tqdm.tqdm(range(len(video)), desc=f"Contact Processor: Processing samples {os.path.join(raw_samples_root_dir, samples_id)}"):
                if len(data_manager._read_contact_processor_results(idx)) > 0:
                    continue
                color_image = video[idx]
                depth_image = depth_npy[idx]  # meters
                contact_processor._process_single_sample(idx, color_image, depth_image)